{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## in section 3\n",
    "\n",
    "working with Pandas\n",
    "\n",
    "built-in summary statistics in Pandas\n",
    "\n",
    "customize visualizations in matplotlib\n",
    "\n",
    "Boston Housing Data Set & Using Kaggle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'><font size='5'>**python libraries**</font>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A library (or a module/package) is a pre-written piece of software\n",
    "\n",
    "we will look at some of the key libraries used in Python for data science.\n",
    "\n",
    "key data science libraries in Python (Numpy, Pandas, Seaborn, Matplotlib, SciPy, Scikit-learn, StatsModels, TensorFlow and Keras\n",
    "\n",
    "some data sets are too large for native pythons built in methods.  \n",
    "\n",
    "The following libraries add scientific computation abilities to Python for working efficiently with larger data sets.\n",
    "\n",
    "NumPy (numerical python) - It provides lots of useful functionality for mathematical operations on vectors and matrices in Python. Matrix computation is the primary strength of NumPy.\n",
    "\n",
    "SciPy - collection of software specifically designed for scientific computing.  SciPy is a library of software for engineering and science applications and contains functions for linear algebra, optimization, integration, and statistics.\n",
    "\n",
    "Statsmodels -- library for Python that enables its users to conduct data exploration via the use of various methods of estimation of statistical models and performing statistical assertions and analysis.  comprehensive set of descriptive statistics\n",
    "\n",
    "Pandas - Pandas is a Python package designed to do work with “relational” data , great tool for data wrangling. It is designed for quick and easy data cleansing, manipulation, aggregation, and visualization.\n",
    "There are two main data structures in the library:\n",
    "\n",
    "“Series” — one-dimensional\n",
    "“Data Frames” - two-dimensional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "MatplotLib - Matplotlib is another SciPy Stack package and a library that is tailored for the generation of simple and powerful visualizations.\n",
    "\n",
    "Matplotlib is a flexible plotting library for creating interactive 2D and 3D plots that can also be saved as manuscript-quality figures. \n",
    "\n",
    "Line plots\n",
    "Scatter plots\n",
    "Bar charts and Histograms\n",
    "Pie charts\n",
    "Stem plots\n",
    "Contour plots\n",
    "Quiver plots\n",
    "Spectrograms\n",
    "\n",
    "The library, however, is pretty low-level which means that you will need to write more code to for advanced visualizations and will generally need more effort.\n",
    "\n",
    "Seaborn -- Seaborn is complimentary to Matplotlib and it specifically targets statistical data visualizations, \n",
    "\n",
    "As Seaborn compliments and extends Matplotlib. If you know Matplotlib, you’ll already have most of Seaborn down. your plots with seaborn will be more attractive, need less time to create and will reveal more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning \n",
    "### Scikit-Learn \n",
    "\n",
    "Scikits provide Scientific \"kits\" on top of SciPy Stack.  image processing and machine learning facilitation. For machine learning, one of the most heavily used package is **scikit-learn**. \n",
    "Scikit-learn (sometimes abbreviated to sklearn) offers a consistent interface to common Machine Learning (ML) algorithms, for problems in classification, regression, clustering and dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning  ( Keras / TensorFlow )\n",
    "\n",
    "For Deep Learning, one of the most popular and convenient libraries for Python is Keras, which builds on top of TensorFlow.\n",
    "\n",
    "### TensorFlow\n",
    "\n",
    "Developed by a team of ML experts at Google, TensorFlow is an open-source library of data flow graph computations, which are fine tuned for heavy duty Machine Learning.\n",
    "\n",
    "The key feature of TensorFlow is its multi-layered nodes system that enables quick training of artificial neural networks on big data. This is the library that powers Google’s voice recognition and object recognition in real time. \n",
    "\n",
    "### Keras\n",
    "\n",
    "Keras is an open-source library for building Neural Networks with a high-level of interface abstraction. The Keras library is written in Python so python developers find it much easier to start coding for deep networks in Keras, than Tensorflow, \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='green'><font size='5'>**Importing Data Using Pandas**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # to import pandas, import as pd for shorthand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Data\n",
    "\n",
    "There are a few main functions for importing data into a pandas DataFrame including:\n",
    "\n",
    "pd.read_csv()<br>\n",
    "pd.read_excel()<br>\n",
    "pd.read_json()<br>\n",
    "pd.DataFrame.from_dict()<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/bp.txt', delimiter='\\t') #delimiter is tabe here\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', nrows=100) # nrows loads that part of a file , can also use skiprows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(0) # removes first row\n",
    "df.head(2)  # limits row count to 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', header=1) # .header specifies where column names are and start the load from that point\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoding\n",
    "\n",
    "Encoding errors like the one above are always frustrating. This has to do with how the strings within the file itself are formatted. The most common encoding other then `utf-8` that you are likely to come across is `latin-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', header=1, encoding='latin-1')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Specific Columns  \n",
    "\n",
    "You can also specify specific columns if you only want to load specific features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', usecols=[0,1,2,5,6], encoding='latin-1')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/ACS_16_5YR_B24011_with_ann.csv', usecols=['GEO.id', 'GEO.id2'], encoding='latin-1')\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting Specific Sheets\n",
    "You can also specify specific sheets for Excel files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_excel('Data/Yelp_Selected_Businesses.xlsx', sheet_name=2)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the name of the sheet itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('Data/Yelp_Selected_Businesses.xlsx', sheet_name='Biz_id_RESDU')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading a Full Workbook and Previewing Sheetnames\n",
    "You can also load an entire excel workbook (which is a collection of spreadsheets) with the `pd.ExcelFile()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workbook = pd.ExcelFile('Data/Yelp_Selected_Businesses.xlsx')\n",
    "workbook.sheet_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = workbook.parse(sheet_name=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Data\n",
    "Once we have data loaded that we may want to export back out, we use the **.to_csv()** or **.to_excel()** methods of any dataframe object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f5a5e934b605>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'NewSavedView.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Notice how we have to pass index=False if we do not want it included in our output\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.to_csv('NewSavedView.csv', index=False) #Notice how we have to pass index=False if we do not want it included in our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
